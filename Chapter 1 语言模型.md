# Chapter 1 语言模型

## 1.1 介绍
在这一章，我们考虑这样一个问题：利用一个由某种语言的样例句子组成的集合建立一个*语言模型(language model)*。语言模型最初是为了解决语音识别(speech recognition)的问题开发的；他们至今在语音识别系统中扮演中心角色。在其他NLP应用中他们也被广泛使用。本章介绍的为了语言模型建模而开发的参数估计(parameter estimation)技术也被应用于其他领域，如本书后面的章节会涉及的标注(tagging)和解析(parsing)问题。

我们的任务是这样的。假设有一个由某种语言中的句子组成的*语料库(corpus)*。比如，几年中纽约时报的文本，或者从网上获取的大规模的文本。给定这样一个语料库，我们要估计语言模型的参数。

一个语言模型的定义如下。首先，定义这门语言中所有单词的集合$\mathcal{V}$。比如，当构建一个英文的语言模型，我们有
$$
\mathcal{V}=\{the, dog, laughs, saw, barks, cat, \dots\}
$$
在实际中，$\mathcal{V}$可能会非常大：它可能包含成千上万的单词。我们假设$\mathcal{V}$是一个有限集合。一个*句子(sentence)*是一个由单词组成的序列
$$
x_1x_2\dots x_n
$$
其中整数$n$满足$n\ge 1$，以及$x_i\in \mathcal{V}$对于$i\in \{1\dots (n-1)\}$，假设$x_n$是一个特殊字符，`STOP`（假设`STOP`不是$\mathcal{V}$中的元素）。我们很快就会看到为什么假设一个句子以`STOP`结尾是很方便的。例句可以是

	the dog barks STOP
	the cat laughs STOP	the cat saw the dog STOP the STOP	cat the dog the STOP	cat cat cat STOP	STOP	...
	
定义$\mathcal{V}^\dagger$是由字典$\mathcal{V}$构成的所有句子的集合：这是一个无穷集合，因为句子可以有任意长度。

给出如下定义：

**定义1（语言模型）** 一个语言模型由一个有穷集合$\mathcal{V}$，和一个满足如下条件的函数$p(x_1, x_2,\dots, x_n)$构成：

1. 对任意的$<x_1\dots x_n>\in \mathcal{V}^\dagger$，$p(x_1, x_2,\dots, x_n)\ge 0$
2. 另外，
$$
\sum_{<x_1\dots x_n>\in \mathcal{V}^\dagger}p(x_1, x_2,\dots, x_n)=1
$$

因此$p(x_1, x_2,\dots, x_n)$是一个定义在$\mathcal{V}^\dagger$的句子上的概率分布。

下面是一个从一个训练语料上学习语言模型的（非常差的）例子。定义$c(x_1\dots x_n)$，作为在我们的训练语料中观测到句子$x_1\dots x_n$的次数，$N$作为训练语料中句子的总数。我们可以定义
$$
p(x_1\dots x_n)=\frac{c(x_1\dots x_n)}{N}
$$

然而这是一个非常差的模型：它会给所有没有在训练语料中出现的句子赋值概率0.因此它无法生成没有在训练数据中见过的句子。本章的关键技术贡献就是介绍能够生成训练数据中没有见过的句子的方法。

第一眼看上去语言模型的建模问题似乎是个很奇怪的工作，那我们为什么还要考虑它？这里有一些理由：

1. 语言模型在一系列应用中都非常有用，最显著的大概就是语音识别和机器翻译。在许多应用中，拥有一个良好的，关于一个句子在一门语言中是否可能出现的“先验(prior)”分布 $p(x_1\dots x_n)$是很有帮助的。比如，在语音识别中，语言模型会和对不同单词的发音进行建模的声学模型(acoustic model)结合起来：一种考虑它的方式是，声学模型生成了大量候选句子及其概率；语言模型接下来被用于基于它们在这门语言中有多大可能成为一个句子而重新排序概率。
2. 用于定义函数$p$和从训练样本中估计结果模型的参数的技术会被用于本课程其他的一些内容中：比如我们接下来会看到的隐马尔可夫模型(hidden Markov models)，和用于自然语言解析的模型。

## 1.2 马尔可夫模型
现在我们转到一个关键问题：给定训练语料，我们如何学习一个函数$p$？这一节描述了一个概率论中的中心想法(central idea[需要更好的翻译])，*马尔可夫模型(Markov models)*；在下一节我们会描述*三元语言模型(trigram language models)*，直接从马尔可夫模型出发建立的一类重要语言模型。

### 1.2.1 定长序列的马尔可夫模型
考虑一个随机变量组成的序列，$X_1, X_2, \dots , X_n$。每一个随机变量可以取有穷集合$\mathcal{V}$中的任意值。目前我们假设这个序列的长度$n$是某个固定的数（比如$n=100$)。在下一节我们将描述如何将这个方法推广到$n$也是一个随机变量的情况中，允许不同的序列有不同的长度。
我们的目标是：我们要对一个定义在序列$x_1\dots x_n$上的概率建模，其中$n\ge 1$而且$x_i\in \mathcal{V}$对与$i=1\dots n$，也就是说，对联合概率
$$
P(X_1=x_1, X_2=x_2, \dots , X_n=x_n)
$$
进行建模。
一共有$|\mathcal{V}|^n$种可能的形式为$x_1\dots x_n$序列：所以很明显，对于合理的$|\mathcal{V}|$和$n$，简单地列出所有$|\mathcal{V}|^n$种可能是不可行的。我们想要构建一个更加紧凑的模型。
在一个一阶马尔可夫过程(first-order Markov process)中，我们做如下假设来简化模型：
$$
P(X_1=x_1, X_2=x_2, \dots , X_n=x_n)
$$
$$
= P(X_1=x_1)\prod^n_{i=2}P(X_i=x_i|X_1=x_1, \dots , X_{i-1}=x_{i-1}) \qquad (1.1)
$$
$$
= P(X_1=x_1)\prod^n_{i=2}P(X_i=x_i|X_{i-1}=x_{i-1}) \qquad (1.2)
$$
在等式1.1中的第一个等号是精确的，通过概率论的链式法则，*任意*分布$P(X_1=x_1\dots X_n=x_n)$可以写成这种形式。所以我们在这一步展开没有做任何假设。然而，在等式1.2中的第二个等号是不精确的：我们要做如下假设，对任意的$i\in\{2\dots n\}$、任意的$x_1\dots x_i$，
$$
P(X_i=x_i|X_1=x_1\dots X_{i-1}=x_{i-1})=P(X_i=x_i|X_{i-1}=x_{i-1})
$$
这是一个（一阶(first-order)）*马尔可夫假设(Markov assumption)*。我们假设序列中第$i$个词只依赖于其前一个词$x_{i-1}$。更正式地讲，我们假设，给定$X_{i-1}$的值，$X_i$与$X_1\dots X_{i-2}$不相关。
在二阶马尔可夫过程中，这将构成三元语言模型的基础，我们建立一个稍微轻一点的假设，即每一个词只与序列中的前两个词有关：
$$
P(X_i=x_i|X_1=x_1\dots X_{i-1}=x_{i-1})
$$
$$
=P(X_i=x_i|X_{i-2}=x_{i-2}, X_{i-1}=x_{i-1})
$$
整个序列的概率可以写成
$$
P(X_1=x_1, X_2=x_2, \dots , X_n=x_n)
$$
$$
=\prod^n_{i=1}P(X_i=x_i|X_{i-2}=x_{i-2}, X_{i-1}=x_{i-1}) \qquad (1.3)
$$
为了方便，我们在定义中假设$x_0=x_{-1}=*$，其中$*$是序列中一个特殊的“开始”符号。

### 1.2.2 变长句子的马尔可夫序列
在上一节，我们假设序列的长度$n$是固定的。然而在许多应用中，长度$n$会发生变化。因此$n$本身是一个随机变量。有许多方法对长度这个变量进行建模：在这一节我们介绍对语言建模最常见的方式。
这个方法很简单：我们假设序列中的第$n$个词，$X_n$，总是等于一个特殊的符号，`STOP`。这个符号只能出现在一个序列的末尾。我们使用与之前完全相同的假设：比日一个在二阶马尔可夫假设下的例子，我们有
$$
P(X_1=x_1, X_2=x_2, \dots , X_n=x_n)=\prod^n_{i=1}P(X_i=x_i|X_{i-2}=x_{i-2}, X_{i-1}=x_{i-1}) \qquad (1.4)
$$
对任意的$n\ge 1$，和任意的$x_1\dots x_n$，其中$x_n=\mathrm{STOP}$，$x_i\in\mathcal{V}$对任意的$i=1\dots(n-1)$。
我们假设了一个二阶马尔可夫过程，根据分布
$$
P(X_i=x_i|X_{i-2}=x_{i-2}, X_{i-1}=x_{i-1})
$$
在每一步生成一个符号$x_i$，它既可以是$\mathcal{V}$中的元素，也可以是符号`STOP`。如果我们生成了符号`STOP`，那就完成了这个队列。否则，我们接着生成这个序列的下一个符号。
稍微形式化点，这个生成句子的过程可以描述为：

1. 初始化$i=1$，$x_0=x_{-1}=*$
2. 根据分布
	$$
	P(X_i=x_i|X_{i-2}=x_{i-2}, X_{i-1}=x_{i-1})
	$$
	生成$x_i$
3. 如果$x_i=\mathrm{STOP}$，返回序列$x_1\dots x_i$，否则，令$i=i+1$，转到步骤2.

因此，我们现在有了一个可以生成变长序列的模型。

## 1.3 三元语言模型
定义语言模型的方式有很多，但是我们在本章将关注于一个尤其重要的例子，三元语言模型。这是上一节介绍的马尔可夫模型在语言建模问题上的一个直接应用。在这一节，我们会给出三元模型的基本定义，讨论三元模型的最大似然参数估计，最后会讨论三元模型的弱点与优势(strengths of weaknesses of trigram models[需要更好的翻译])。

### 1.3.1 基本定义
就像在马尔可夫模型中，我们把每个句子件模型一个包含$n$个随机变量的序列，$X_1, X_2, \dots , X_n$。长度$n$本身也是一个随机变量（不同的句子可能有不同的$n$）。我们总是有$X_n=\mathrm{STOP}$。在二阶马尔可夫模型下，任意句子$x_1\dots x_n$的概率是
$$
P(X_1=x_1, X_2=x_2, \dots , X_n=x_n)=\prod^n_{i=1}P(X_i=x_i|X_{i-2}=x_{i-2}, X_{i-1}=x_{i-1})
$$
如之前一样，我们假设$x_0=x_{-1}=*$。
我们假设对任意的$i$，任意$x_{i-2}, x_{i-1}, x_i$，
$$
P(X_i=x_i|X_{i-2}=x_{i-2}, X_{i-1}=x_{i-1})=q(x_i|x_{i-2}, x_{i-1})
$$
其中对任意的$(u, v, w)$的$q(w|u, v)$是模型的参数。我们很快会看到如何从训练语料中获得$q(w|u, v)$的估计。我们的模型